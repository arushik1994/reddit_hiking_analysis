{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re #for extracting place names from titles\n",
    "\n",
    "import accesses #local file with API certs and database passwords.\n",
    "\n",
    "#for connecting to databases\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import json #for parsing the return from the Google API\n",
    "import urllib #for passing info to the Google API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How well did our code work at extracting, converting and geocoding our locations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had to pull the data in multiple batches, and not all of the data was returned sequentially by PushShift's API.  Therefore, I'll make sure that every unique ID that was scraped from Reddit that was successfully extracted and converted through my processes was passed to the GoogleMaps API.  We'll then run through this notebook one more time to ensure every unique ID is accounted for.  I also updated the code to update a NULL value into the database if geocoding failed so that we could track them as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebuild raw_reddit, extracted, converted, and geocoded dataframes for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat a connection and cursor directly to the database using psycopg2.\n",
    "conn = psycopg2.connect(host=\"localhost\",database=\"reddit\", user=accesses.db_user, \n",
    "                        password=accesses.db_pw)\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = conn.cursor()\n",
    "df_raw_reddit = pd.read_sql('select * from raw_reddit', conn, index_col='id')\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_1 = r'((?:[A-Z]\\w+\\.*,*\\s*\\n*){2,})'\n",
    "\n",
    "df_extracted = df_raw_reddit['title'].str.extractall(pat_1).unstack()\n",
    "# to return the first element, the dataframe\n",
    "df_extracted = df_extracted[0]\n",
    "df_extracted = df_extracted.rename(columns = {0:'extracted_0',1:'extracted_1',\n",
    "                                  2:'extracted_2',3:'extracted_3',\n",
    "                                  4:'extracted_4'})\n",
    "places = pd.merge(df_raw_reddit, df_extracted, how='left', left_index=True,\n",
    "                      right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_address(address):\n",
    "    converted_address = ''\n",
    "    try: \n",
    "        for word in address.split():\n",
    "            converted_address += (word + '+')        \n",
    "        return converted_address[:-1]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "places['converted_0'] = places['extracted_0'].apply(convert_address)\n",
    "places['converted_1'] = places['extracted_1'].apply(convert_address)\n",
    "\n",
    "places = places.replace({None:np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33979\n",
      "33952\n"
     ]
    }
   ],
   "source": [
    "# During testing, multiple runs introduced several duplicates.  Since this\n",
    "# class focuses on Python, I handled the duplicates here rather than in \n",
    "# the database itself.\n",
    "\n",
    "c = conn.cursor()\n",
    "# select all rows from the geocoded_addresses table\n",
    "df_geocoded = pd.read_sql('select * from geocoded_addresses', conn)\n",
    "# get original length\n",
    "print(len(df_geocoded))\n",
    "# drop all duplicates by ID and set index to the id\n",
    "df_geocoded.drop_duplicates('id', inplace=True)\n",
    "df_geocoded.set_index('id', inplace=True)\n",
    "# print length to see how many we removed\n",
    "print(len(df_geocoded))\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_geo_dict(df):\n",
    "    geo_dict_list = []\n",
    "    error_list = {}\n",
    "    for row in df.iterrows():\n",
    "        uid = row[0]\n",
    "        data = (row[1][0])\n",
    "        geo_dict = {}\n",
    "        geo_dict['id'] = uid\n",
    "        \n",
    "        try:\n",
    "            geo_dict['lat'] = data['results'][0]['geometry']['location']['lat']\n",
    "            geo_dict['lon'] = data['results'][0]['geometry']['location']['lng']\n",
    "            for component in data['results'][0]['address_components']:\n",
    "                geo_dict[component['types'][0]] = component['long_name']\n",
    "            geo_dict_list.append(geo_dict)\n",
    "        \n",
    "        except: \n",
    "            error_list[uid]=data\n",
    "        \n",
    "    return (geo_dict_list, error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, errors = build_geo_dict(df_geocoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_full = pd.DataFrame(results)\n",
    "final_full.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(final_full, places, right_index=True, left_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = merged[['country','administrative_area_level_1', 'score', 'title', 'extracted_0',\n",
    "                'dt_time','lat', 'lon']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows scraped from reddit: 43900\n",
      "Rows extracted by RegEx: 33952\n",
      "Rows from geocoded table: 33952\n",
      "Rows geocoded accurately: 30940\n",
      "Rows with geocoding errors 3012\n",
      "Rows in final df: 30940\n"
     ]
    }
   ],
   "source": [
    "print('Rows scraped from reddit:', len(df_raw_reddit))\n",
    "print('Rows extracted by RegEx:', len(df_extracted))\n",
    "print('Rows from geocoded table:', len(df_geocoded))\n",
    "print('Rows geocoded accurately:', len(results))\n",
    "print('Rows with geocoding errors', len(errors))\n",
    "print('Rows in final df:', len(final_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are all the rows extracted from the RegEx in the Geocoded table?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were originally thousands of rows that were successfully extracted from the raw reddit titles but had not been successfully geocoded.  To make sure we processed all of our locations, we will take the index of all the successful extractions, iterate through each one and see if that index is in the geocoded index, and if it is not we will add it to a new list called 'missing_ids'.  We'll create a new dataframe from the original places dataframe using the 'missing_ids' to subset the full dataframe.  \n",
    "\n",
    "This new dataframe has just the ids that were successfully extracted and converted, but never geocoded.  We can now run our geocoding function against these ids.  We'll also use an updated function that writes None to the database if the GoogleMaps API rejects it, which we weren't doing in the first scrape.  This will ensure that future run-throughs of the process wont keep on trying to run the same places through the API that cause errors.  We'll capture all of these errors in our geo_dict building funtion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_index = df_extracted.index\n",
    "geocoded_index = df_geocoded.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ids=[]\n",
    "for i in extracted_index:\n",
    "    if i not in geocoded_index:\n",
    "        missing_ids.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprocessing = places.loc[missing_ids,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reprocess Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geocode_to_db(df):\n",
    "    \n",
    "    sql_insert_geocode = \"\"\"INSERT INTO geocoded_addresses(id, results) VALUES(%s,%s)\"\"\"  \n",
    "    sql_insert_error = \"\"\"INSERT INTO geocoded_addresses(id, results) VALUES(%s,%s)\"\"\"  \n",
    "    api_key = accesses.google_api\n",
    "    url = 'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "    \n",
    "    #start = find_start('geocoded_addresses')\n",
    "    #end = start + chunk_size\n",
    "    # If the end of the chunk is greater than the index (accessed here as 'name') of \n",
    "    # the last element in the dataframe, use the last element's index as the endpoint.\n",
    "    # this will prevent 'out of range' errors while ensureing we geocode all data available.\n",
    "    #if end > places.iloc[-1].name:\n",
    "    #    end = places.iloc[-1].name\n",
    "    #print(\"Will start processing at loc {}.  Will End processing at {}\".format(start, end))\n",
    "    \n",
    "    for name, row in df.iterrows():\n",
    "        c = conn.cursor()\n",
    "        address = row[0] \n",
    "        uid = name # unique id\n",
    "        try:\n",
    "            if pd.notna(address): \n",
    "                url_address_api = '{}address={}&key={}'.format(url, address, api_key)\n",
    "                \n",
    "                with urllib.request.urlopen(url_address_api) as response: \n",
    "                    js = (json.loads(response.read()))\n",
    "                # Depsite multiple attempts, I had to load the JSON from the API and \n",
    "                # then dump it back into the PostGRES database.  When I tried to write\n",
    "                # the JSON directly to the database, I kept getting errors.\n",
    "                c.execute(sql_insert_geocode, (uid, json.dumps(js))) \n",
    "                conn.commit()\n",
    "                print('Success! ID: ', uid)\n",
    "                \n",
    "            else: print('Row was blank.  Continuing.  ID: ', uid)\n",
    "        except: \n",
    "            c.execute(sql_insert_error, (uid,  None,))\n",
    "            print('Error in geocoding.  ID: ', uid)\n",
    "        c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "geocode_to_db(reprocessing['converted_0'].to_frame())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
