{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python-Reddit API Wrapper\n",
    "import praw \n",
    "from psaw import PushshiftAPI\n",
    "\n",
    "#for connecting to databases\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import json #for parsing the return from the Google API\n",
    "import urllib #for passing info to the Google API\n",
    "\n",
    "import accesses #local file with API certs and database passwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Connections to Reddit, PushShift, and PostGres Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat a connection and cursor directly to the database using psycopg2.\n",
    "conn = psycopg2.connect(host=\"localhost\",database=\"reddit\", user=accesses.db_user, \n",
    "                        password=accesses.db_pw)\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to rollback the cursor as neccessary.\n",
    "# Keep commented out unless needed.\n",
    "#conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Create the API credential variables using the local accesses file, and use those\n",
    "# to create an instance of Reddit using the Python-Reddit API Wrapper (PRAW).\n",
    "\n",
    "client_id=accesses.client_id\n",
    "client_secret=accesses.client_secret\n",
    "user_agent=accesses.user_agent\n",
    "\n",
    "reddit = praw.Reddit(client_id=client_id,\n",
    "                     client_secret=client_secret,\n",
    "                     user_agent=user_agent)\n",
    "\n",
    "# Test the Reddit instance.  Should return 'True'.\n",
    "print(reddit.read_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI\n",
    "# use the reddit instance created with PRAW to connect to PushshiftAPI\n",
    "api = PushshiftAPI(reddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape all Reddit IDs from Hiking using PushShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this code to download all submission IDs from January 2017 to October 2019\n",
    "start_epoch=int(dt.datetime(2017, 1, 1).timestamp())\n",
    "end_epoch=int(dt.datetime(2019, 10, 1).timestamp())\n",
    "\n",
    "# Save all results from the API to a list.  This is usually fast (less than 10 mins)\n",
    "# So I did not write a database commit.  Additionally, the PushShift API only allows\n",
    "# batch searching.\n",
    "submission_results = list(api.search_submissions(after=start_epoch,before=end_epoch,\n",
    "                                     subreddit='hiking'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will save off the Reddit Ids into a csv if we need them later.\n",
    "submission_results_df = pd.DataFrame(submission_results)\n",
    "submission_results_df.to_csv('{}_to_{}.csv'.fomrat(start_epoch, end_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Reddit API to get information from each Reddit ID, and save to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"raw_reddit\" table in the \"reddit\" database.\n",
    "c = conn.cursor()\n",
    "c.execute(\"\"\"CREATE TABLE IF NOT EXISTS public.raw_reddit\n",
    "(\n",
    "    id serial,\n",
    "    reddit_id text,\n",
    "    title text,\n",
    "    score int, \n",
    "    num_comments int,\n",
    "    int_time int,\n",
    "    dt_time timestamp\n",
    ");\"\"\")\n",
    "conn.commit()\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each reddit ID in the list of results, use the reddit API to get the\n",
    "# title, score, num_comments, int_time (default returned by reddit) and the\n",
    "# converted day time.\n",
    "c = conn.cursor()\n",
    "sql_insert = \"\"\"INSERT INTO raw_reddit(reddit_id, title, score, num_comments, \n",
    "int_time, dt_time) VALUES(%s,%s,%s,%s,%s,%s)\"\"\"  \n",
    "\n",
    "for s in submission_results:\n",
    "    try:\n",
    "        reddit_id = s.id\n",
    "        title = praw.models.Submission(reddit,id=s.id).title\n",
    "        score = praw.models.Submission(reddit,id=s.id).score\n",
    "        num_comments = praw.models.Submission(reddit, id=s.id).score\n",
    "        int_time = praw.models.Submission(reddit, id=s.id).created\n",
    "        dt_time = dt.datetime.utcfromtimestamp(int_time)\n",
    "        c.execute(sql_insert, (reddit_id,title,score,num_comments,int_time,dt_time))\n",
    "        conn.commit()\n",
    "        #print(reddit_id,title,score,num_comments,int_time,dt_time)\n",
    "    except:\n",
    "        print('Oops, failure')\n",
    "        print(s.id)\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops the raw_reddit table.  \n",
    "# DO NOT RUN THIS UNLESS YOU WANT TO RE-SCRAPE REDDIT!!!\n",
    "\n",
    "#c.execute(\"\"\"DROP table public.raw_reddit;\"\"\")\n",
    "#conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve all reddit posts and assocated data from database, and condition it for GoogleMaps API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = conn.cursor()\n",
    "df_raw_reddit_full = pd.read_sql('select * from raw_reddit', conn, index_col='id')\n",
    "df_raw_reddit = df_raw_reddit_full.drop_duplicates('title')\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify place names, we will use a regular expression to find likely place names. This regular expression first looks for a non-capture group \n",
    "\n",
    "-beginning with a capital letter\n",
    "\n",
    "-followed by one or more word element\n",
    "\n",
    "-followed by zero or more periods (for abbreviations)\n",
    "\n",
    "-followed by zero or more commas (since commas are often used to separate address pieces)\n",
    "\n",
    "-followed by zero or more spaces\n",
    "\n",
    "-followed by zero or more line breaks\n",
    "and then captures whenever two or more of these patterns are identified.  There are several weaknesses with this approach.  First, place names that are not consistently capitalized like \"Mount vernon, Virginia\" will not be captured.  Additionally, many foreign place names such as \"Playa del Mar\" are not captured.  Many non-English words also negatively impact the Google API.  However, this simple approach is highly successful against the data at hand which is usually well-structured and consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_1 = r'((?:[A-Z]\\w+\\.*,*\\s*\\n*){2,})'\n",
    "\n",
    "places = df_raw_reddit['title'].str.extractall(pat_1).unstack()\n",
    "# to return the first element, the dataframe\n",
    "places = places[0]\n",
    "places = places.rename(columns = {0:'extracted_0',1:'extracted_1'})\n",
    "places = pd.merge(df_raw_reddit, places, how='left', left_index=True,\n",
    "                      right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The API expects no spaces and words concatanated with a '+', which \n",
    "# is what this function does.\n",
    "def convert_address(address):\n",
    "    converted_address = ''\n",
    "    try: \n",
    "        for word in address.split():\n",
    "            converted_address += (word + '+')        \n",
    "        return converted_address[:-1]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "places['converted_0'] = places['extracted_0'].apply(convert_address)\n",
    "places['converted_1'] = places['extracted_1'].apply(convert_address)\n",
    "places = places.replace({None:np.nan})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of using GoogleMap API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API is accessed by passing the below url, the API key, and the converted address together.  The result is a variable length JSON object.  As shown below, we can use a dictionary to store the most relevenat metadata into an easily parsable object, no matter the length of the JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = 'George+Washington+University'\n",
    "api_key = accesses.google_api\n",
    "url = 'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "url_address_api = '{}address={}&key={}'.format(url, address, api_key)\n",
    "\n",
    "geo_dict = {}\n",
    "geo_dict['address'] = address\n",
    "try:\n",
    "    with urllib.request.urlopen(url_address_api) as response: \n",
    "        js = json.loads(response.read())\n",
    "    geo_dict['lat'] = js['results'][0]['geometry']['location']['lat']\n",
    "    geo_dict['lon'] = js['results'][0]['geometry']['location']['lng']\n",
    "    for component in js['results'][0]['address_components']:\n",
    "        geo_dict[component['types'][0]] = component['long_name']\n",
    "    \n",
    "except:\n",
    "    print('Error in Geocoding.', address, ' not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [{'address_components': [{'long_name': '2121',\n",
       "     'short_name': '2121',\n",
       "     'types': ['street_number']},\n",
       "    {'long_name': 'I Street Northwest',\n",
       "     'short_name': 'I St NW',\n",
       "     'types': ['route']},\n",
       "    {'long_name': 'Northwest Washington',\n",
       "     'short_name': 'Northwest Washington',\n",
       "     'types': ['neighborhood', 'political']},\n",
       "    {'long_name': 'Washington',\n",
       "     'short_name': 'Washington',\n",
       "     'types': ['locality', 'political']},\n",
       "    {'long_name': 'District of Columbia',\n",
       "     'short_name': 'DC',\n",
       "     'types': ['administrative_area_level_1', 'political']},\n",
       "    {'long_name': 'United States',\n",
       "     'short_name': 'US',\n",
       "     'types': ['country', 'political']},\n",
       "    {'long_name': '20052', 'short_name': '20052', 'types': ['postal_code']}],\n",
       "   'formatted_address': '2121 I St NW, Washington, DC 20052, USA',\n",
       "   'geometry': {'location': {'lat': 38.8997145, 'lng': -77.0485992},\n",
       "    'location_type': 'ROOFTOP',\n",
       "    'viewport': {'northeast': {'lat': 38.9010634802915,\n",
       "      'lng': -77.04725021970849},\n",
       "     'southwest': {'lat': 38.8983655197085, 'lng': -77.0499481802915}}},\n",
       "   'place_id': 'ChIJpXo8DrG3t4kRCKgn3Bv0e9o',\n",
       "   'plus_code': {'compound_code': 'VXX2+VH Washington, District of Columbia, United States',\n",
       "    'global_code': '87C4VXX2+VH'},\n",
       "   'types': ['establishment', 'point_of_interest', 'university']}],\n",
       " 'status': 'OK'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'address': 'George+Washington+University',\n",
       " 'lat': 38.8997145,\n",
       " 'lon': -77.0485992,\n",
       " 'street_number': '2121',\n",
       " 'route': 'I Street Northwest',\n",
       " 'neighborhood': 'Northwest Washington',\n",
       " 'locality': 'Washington',\n",
       " 'administrative_area_level_1': 'District of Columbia',\n",
       " 'country': 'United States',\n",
       " 'postal_code': '20052'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the goal is to see if we can find a way to do this for everything.  The problem is that 'address components' is variable.  In some responses, there are only two components.  In otheres there are five.  Additionally, the different levels repersent different elements depending on where the location is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the conditioned data from the title as an input to Google's Geocoding API to return coordinates and location metadata.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are using the free tier of Google's API, we can only process 40,000 geocodings a month.  Therefore, we will want to save every successful call into the database in its raw form so we do not have to get it back from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"geocoded_addresses\" table in the \"reddit\" database.\n",
    "c = conn.cursor()\n",
    "c.execute(\"\"\"CREATE TABLE IF NOT EXISTS public.geocoded_addresses\n",
    "(\n",
    "    id int,\n",
    "    results json\n",
    ");\"\"\")\n",
    "conn.commit()\n",
    "c.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops the geocoded_addresses table.  \n",
    "# Keep commented out.\n",
    "#c = conn.cursor()\n",
    "#c.execute(\"\"\"DROP table public.geocoded_addresses;\"\"\")\n",
    "#conn.commit()\n",
    "#c.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to check highest row:\n",
    "def find_start(table_name, conn=conn):\n",
    "    c = conn.cursor()\n",
    "    query = \"select max(id) from {}\".format(table_name)\n",
    "    c.execute(query)\n",
    "    start = (c.fetchone()[0])\n",
    "    c.close\n",
    "    if start == None:\n",
    "        start = 0\n",
    "    return start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geocode_to_db(df, chunk_size):\n",
    "    \n",
    "    sql_insert_geocode = \"\"\"INSERT INTO geocoded_addresses(id, results) VALUES(%s,%s)\"\"\"  \n",
    "    api_key = accesses.google_api\n",
    "    url = 'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "    \n",
    "    start = find_start('geocoded_addresses')\n",
    "    end = start + chunk_size\n",
    "    # If the end of the chunk is greater than the index (accessed here as 'name') of \n",
    "    # the last element in the dataframe, use the last element's index as the endpoint.\n",
    "    # this will prevent 'out of range' errors while ensureing we geocode all data available.\n",
    "    if end > places.iloc[-1].name:\n",
    "        end = places.iloc[-1].name\n",
    "    print(\"Will start processing at loc {}.  Will End processing at {}\".format(start, end))\n",
    "    \n",
    "    for name, row in df[start:end].iterrows():\n",
    "        c = conn.cursor()\n",
    "        address = row[0] \n",
    "        uid = name # unique id\n",
    "        try:\n",
    "            if pd.notna(address): \n",
    "                url_address_api = '{}address={}&key={}'.format(url, address, api_key)\n",
    "                \n",
    "                with urllib.request.urlopen(url_address_api) as response: \n",
    "                    js = (json.loads(response.read()))\n",
    "                # Depsite multiple attempts, I had to load the JSON from the API and \n",
    "                # then dump it back into the PostGRES database.  When I tried to write\n",
    "                # the JSON directly to the database, I kept getting errors.\n",
    "                c.execute(sql_insert_geocode, (uid, json.dumps(js))) \n",
    "                conn.commit()\n",
    "                print('Success! ID: ', uid)\n",
    "                \n",
    "            else: print('Row was blank.  Continuing.  ID: ', uid)\n",
    "        except: print('Error in geocoding.  ID: ', uid)\n",
    "        c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "geocode_to_db((converted['converted_0'].to_frame()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
